import faiss
import numpy as np
import os
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI
import re
from file_loads import load_docs
from fastapi import FastAPI, Request, Query, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
import uvicorn
import glob
from fastapi.middleware.cors import CORSMiddleware
from document_loader import DocumentLoader
from pathlib import Path
import json
import urllib.parse

from database import db
import asyncio
from dotenv import load_dotenv
import time
from datetime import datetime
from functools import lru_cache
import hashlib
import pickle
from typing import Dict, List, Tuple, Optional
from contextlib import asynccontextmanager
import requests
import aiohttp
from mcp_api import router as mcp_router
import sqlite3

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def get_env_or_default(key: str, default: str = "") -> str:
    """è·å–ç¯å¢ƒå˜é‡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›é»˜è®¤å€¼"""
    return os.getenv(key, default)

def get_env_int(key: str, default: int = 0) -> int:
    """è·å–ç¯å¢ƒå˜é‡å¹¶è½¬æ¢ä¸ºæ•´æ•°"""
    try:
        return int(os.getenv(key, str(default)))
    except (ValueError, TypeError):
        return default

# ç›´æ¥ä»ç¯å¢ƒå˜é‡æ„å»ºæ¨¡å‹é…ç½®
def get_model_config():
    """ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹é…ç½®"""
    return {
        "glm-4-plus": {
            "model": get_env_or_default("GLM_4_PLUS_MODEL", "glm-4-plus"),
            "api_key": get_env_or_default("GLM_4_PLUS_API_KEY", ""),
            "api_base": get_env_or_default("GLM_4_PLUS_API_BASE", "https://open.bigmodel.cn/api/paas/v4/")
        },
        "deepseek": {
            "model": get_env_or_default("DEEPSEEK_MODEL", "DeepSeek-R1"),
            "api_key": get_env_or_default("DEEPSEEK_API_KEY", ""),
            "api_base": get_env_or_default("DEEPSEEK_API_BASE", "https://api.deepseek.com")
        },
        "qwen": {
            "model": get_env_or_default("QWEN_MODEL", "qwen3-235b-a22b"),
            "api_key": get_env_or_default("QWEN_API_KEY", ""),
            "api_base": get_env_or_default("QWEN_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")
        },
        "claude3.7": {
            "model": get_env_or_default("CLAUDE_MODEL", "claude-3-7-sonnet"),
            "api_key": get_env_or_default("CLAUDE_API_KEY", ""),
            "api_base": get_env_or_default("CLAUDE_API_BASE", "https://api.anthropic.com/v1")
        }
    }

def get_bocha_config():
    """ä»ç¯å¢ƒå˜é‡è·å–Bochaé…ç½®"""
    return {
        "api_key": get_env_or_default("BOCHA_API_KEY", ""),
        "api_base": get_env_or_default("BOCHA_API_BASE", "https://api.bochaai.com/v1/web-search"),
        "timeout": get_env_int("BOCHA_TIMEOUT", 30)
    }

# è·å–é…ç½®
MODEL_CONFIG = get_model_config()
BOCHA_CONFIG = get_bocha_config()
DEFAULT_MODEL = get_env_or_default("DEFAULT_MODEL", "glm-4-plus")

def load_model():
    """åŠ è½½æˆ–åˆå§‹åŒ–æ¨¡å‹"""
    local_model_path = 'local_m3e_model'
    if os.path.exists(local_model_path):
        print(f"ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {local_model_path}")
        model = SentenceTransformer(local_model_path)
    else:
        print(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä»ç½‘ç»œåŠ è½½: moka-ai/m3e-base")
        model = SentenceTransformer('moka-ai/m3e-base')
        print(f"ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°: {local_model_path}")
        model.save(local_model_path)
    # å¼ºåˆ¶ä½¿ç”¨GPU
    try:
        model = model.to('cuda')
        print('æ¨¡å‹å·²è½¬ç§»åˆ°GPU')
    except Exception as e:
        print('æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œä½¿ç”¨CPU')
    return model

def chunk_document(text, max_chars=500, overlap=100, is_excel=False, file_name=None):
    """
    å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆè¾ƒå°çš„å—ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£ç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯æ€§
    
    å‚æ•°:
        text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
        max_chars: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
        overlap: ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        is_excel: æ˜¯å¦ä¸ºExcelæ–‡ä»¶
        file_name: æ–‡ä»¶å
    
    è¿”å›:
        chunks: åˆ‡åˆ†åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    # æ ¹æ®æ–‡ä»¶ç±»å‹è®¾ç½®åˆ†å—å‚æ•°
    if is_excel:
        max_chars = 1000
        overlap = 0
    
    # æ–‡ä»¶åå‰ç¼€
    file_prefix = ""
    if file_name:
        file_prefix = f"[æ–‡ä»¶ï¼š{file_name}]\n"
    
    if len(text) <= max_chars:
        # å¦‚æœæ•´ä¸ªæ–‡æœ¬å°äºä¸€ä¸ªå—çš„å¤§å°ï¼Œç›´æ¥æ·»åŠ æ–‡ä»¶åå‰ç¼€è¿”å›
        if file_name and not text.startswith(file_prefix):
            return [f"{file_prefix}{text}"]
        return [text]
    
    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„æ–‡ä»¶åå‰ç¼€ï¼Œä»¥é¿å…é‡å¤
    if file_name and text.startswith(file_prefix):
        text = text[len(file_prefix):]
    
    chunks = []
    start = 0
    last_end = 0

    while start < len(text):
        end = min(start + max_chars, len(text))
        
        if end < len(text):
            sentence_ends = [
                m.end() for m in re.finditer(r'[ã€‚ï¼ï¼Ÿ.!?]\s*', text[start:end])
            ]
            
            if sentence_ends:
                end = start + sentence_ends[-1]
            else:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åœ¨å•è¯æˆ–æ ‡ç‚¹å¤„åˆ‡åˆ†
                last_space = text[start:end].rfind(' ')
                last_punct = max(text[start:end].rfind('ï¼Œ'), text[start:end].rfind(','))
                cut_point = max(last_space, last_punct)
                
                if cut_point > 0:
                    end = start + cut_point + 1
        
        # æ¯ä¸ªchunkéƒ½æ·»åŠ æ–‡ä»¶åå‰ç¼€
        chunk = text[start:end]
        if file_name:
            chunk = f"{file_prefix}{chunk}"
        
        chunks.append(chunk)
        
        if end <= last_end:
            end = min(last_end + 1, len(text))
            chunk = text[start:end]
            if file_name:
                chunk = f"{file_prefix}{chunk}"
            chunks[-1] = chunk
            
            if end >= len(text):
                break
        
        last_end = end
        start = end - overlap
        
        if start < 0:
            start = 0
            
        if start >= end:
            start = end
            
        if start >= len(text):
            break
    
    return chunks

# æ·»åŠ ç¼“å­˜ç›¸å…³çš„å¸¸é‡å’Œå˜é‡
CACHE_DIR = Path("cache")
VECTOR_CACHE_FILE = CACHE_DIR / "vector_cache.pkl"
EMBEDDING_CACHE_FILE = CACHE_DIR / "embedding_cache.pkl"

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
CACHE_DIR.mkdir(exist_ok=True)

class VectorCache:
    def __init__(self):
        self.cache: Dict[str, np.ndarray] = {}
        self.load_cache()

    def load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if VECTOR_CACHE_FILE.exists():
            try:
                with open(VECTOR_CACHE_FILE, 'rb') as f:
                    self.cache = pickle.load(f)
            except Exception:
                self.cache = {}

    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(VECTOR_CACHE_FILE, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception:
            pass

    def get(self, key: str) -> Optional[np.ndarray]:
        """è·å–ç¼“å­˜çš„å‘é‡"""
        return self.cache.get(key)

    def set(self, key: str, vector: np.ndarray):
        """è®¾ç½®å‘é‡ç¼“å­˜"""
        self.cache[key] = vector

class EmbeddingCache:
    def __init__(self):
        self.cache: Dict[str, np.ndarray] = {}
        self.load_cache()

    def load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if EMBEDDING_CACHE_FILE.exists():
            try:
                with open(EMBEDDING_CACHE_FILE, 'rb') as f:
                    self.cache = pickle.load(f)
            except Exception:
                self.cache = {}

    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(EMBEDDING_CACHE_FILE, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception:
            pass

    def get(self, key: str) -> Optional[np.ndarray]:
        """è·å–ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        return self.cache.get(key)

    def set(self, key: str, vector: np.ndarray):
        """è®¾ç½®åµŒå…¥å‘é‡ç¼“å­˜"""
        self.cache[key] = vector

# åˆ›å»ºç¼“å­˜å®ä¾‹
vector_cache = VectorCache()
embedding_cache = EmbeddingCache()

@lru_cache(maxsize=1000)
def get_cached_embeddings(text: str) -> np.ndarray:
    """ç¼“å­˜æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
    # é¦–å…ˆæ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜
    cache_key = hashlib.md5(text.encode()).hexdigest()
    cached_vector = embedding_cache.get(cache_key)
    if cached_vector is not None:
        return cached_vector

    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œè®¡ç®—æ–°çš„åµŒå…¥å‘é‡
    vector = model.encode(text, convert_to_tensor=False)
    
    # ä¿å­˜åˆ°æŒä¹…åŒ–ç¼“å­˜
    embedding_cache.set(cache_key, vector)
    return vector

def get_query_hash(query):
    """ç”ŸæˆæŸ¥è¯¢çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
    return hashlib.md5(query.encode()).hexdigest()

def get_embeddings(model, texts):
    """
    è·å–æ–‡æœ¬åµŒå…¥å‘é‡
    
    å‚æ•°:
        model: æ¨¡å‹å®ä¾‹
        texts: æ–‡æœ¬åˆ—è¡¨
    
    è¿”å›:
        numpyæ•°ç»„å½¢å¼çš„åµŒå…¥å‘é‡
    """
    embeddings = model.encode(texts, normalize_embeddings=True)
    return np.array(embeddings)

def create_or_load_index(model, all_chunks, document_to_chunks, chunks_to_document):
    """åˆ›å»ºæˆ–åŠ è½½FAISSç´¢å¼•ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
    index_path = 'faiss_index.faiss'
    chunks_map_path = 'chunks_mapping.npy'  

    if os.path.exists(index_path):
        print(f"ä»æœ¬åœ°åŠ è½½ç´¢å¼•: {index_path}")
        index = faiss.read_index(index_path)
        return index
    else:
        print("ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ")
        return None

# æ·»åŠ ç³»ç»ŸçŠ¶æ€ç®¡ç†ç±»
class SystemState:
    def __init__(self):
        self.initialized = False
        self.last_check = 0
        self.check_interval = 60  # 60ç§’æ£€æŸ¥ä¸€æ¬¡
    
    def needs_check(self):
        current_time = time.time()
        return not self.initialized or (current_time - self.last_check) > self.check_interval
    
    def mark_initialized(self):
        self.initialized = True
        self.last_check = time.time()

# åˆ›å»ºç³»ç»ŸçŠ¶æ€å®ä¾‹
system_state = SystemState()

# ä¼˜åŒ–æŸ¥è¯¢åµŒå…¥ç¼“å­˜
@lru_cache(maxsize=1000)
def get_cached_query_embedding(query_hash: str, query: str) -> np.ndarray:
    """ç¼“å­˜æŸ¥è¯¢çš„åµŒå…¥å‘é‡"""
    return model.encode(query, convert_to_tensor=False)

def get_query_embedding_cached(query: str) -> np.ndarray:
    """è·å–ç¼“å­˜çš„æŸ¥è¯¢åµŒå…¥å‘é‡"""
    query_hash = hashlib.md5(query.encode()).hexdigest()
    return get_cached_query_embedding(query_hash, query)

def get_related_documents(model, index, query, all_chunks, k=5):
    """ä¼˜åŒ–çš„æ–‡æ¡£æ£€ç´¢å‡½æ•°"""
    if not all_chunks:
        return [], []
        
    # ç¡®ä¿kä¸è¶…è¿‡chunksæ•°é‡
    k = min(k, len(all_chunks))
    
    try:
        # ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥å‘é‡è®¡ç®—
        query_embedding = get_query_embedding_cached(query)
        query_embedding = query_embedding.reshape(1, -1)
        
        # ä¼˜åŒ–FAISSæœç´¢å‚æ•°
        if hasattr(index, 'nprobe') and hasattr(index, 'ntotal'):
            # åŠ¨æ€è°ƒæ•´nprobeä»¥å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§
            index.nprobe = min(32, max(1, index.ntotal // 100))
        
        # ä½¿ç”¨faissæœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
        distances, indices = index.search(query_embedding, k=k)
        
        # æ£€æŸ¥indicesæ˜¯å¦ä¸ºç©º
        if len(indices) == 0 or len(indices[0]) == 0:
            return [], []
            
        # æ‰¹é‡å¤„ç†æ–‡æ¡£å—ï¼Œå‡å°‘å¾ªç¯å¼€é”€
        context = []
        valid_indices = []
        processed_files = set()  # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„Excelæ–‡ä»¶
        
        # ç›´æ¥å¤„ç†ç´¢å¼•ï¼Œé¿å…å¤æ‚çš„æ‰¹é‡é€»è¾‘
        for idx in indices[0]:
            if not (0 <= idx < len(all_chunks)):
                continue
                
            chunk = all_chunks[idx]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Excelæ–‡ä»¶æ ‡è®°
            if "[æ–‡ä»¶ï¼š" in chunk and any(ext in chunk for ext in ['.xlsx', '.xls']):
                # æå–æ–‡ä»¶å
                file_match = re.search(r'\[æ–‡ä»¶ï¼š(.*?)\]', chunk)
                if file_match:
                    file_name = file_match.group(1)
                    if file_name not in processed_files:
                        # å¦‚æœæ˜¯Excelæ–‡ä»¶ä¸”æœªå¤„ç†è¿‡ï¼ŒåŠ è½½æ–‡ä»¶å†…å®¹
                        try:
                            file_path = os.path.join('uploads', file_name)
                            doc_info = document_loader.load_document(file_path)
                            if doc_info['file_type'] == 'excel':
                                # ç®€åŒ–Excelå¤„ç†ï¼Œåªå–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                                if doc_info['sheet_contents']:
                                    first_sheet = list(doc_info['sheet_contents'].items())[0]
                                    context.append(f"å·¥ä½œè¡¨: {first_sheet[0]}\n{first_sheet[1]}")
                                valid_indices.append(idx)
                                processed_files.add(file_name)
                        except Exception:
                            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å—
                            context.append(chunk)
                            valid_indices.append(idx)
                    else:
                        # éExcelæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å—
                        context.append(chunk)
                        valid_indices.append(idx)
            else:
                # éExcelæ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹å—
                context.append(chunk)
                valid_indices.append(idx)
                
        return context, valid_indices
    except Exception as e:
        print(f"æ–‡æ¡£æ£€ç´¢é”™è¯¯: {e}")
        return [], []

async def web_search(query: str) -> str:
    """æ‰§è¡Œç½‘ç»œæœç´¢"""
    try:
        # æ„å»ºè¯·æ±‚
        api_url = BOCHA_CONFIG["api_base"]
        headers = {
            "Authorization": f"Bearer {BOCHA_CONFIG['api_key']}",
            "Content-Type": "application/json"
        }
        payload = {
            "query": query,
            "summary": True,
            "count": 10,
            "page": 1
        }
        
        # å‘é€è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(api_url, headers=headers, json=payload, timeout=BOCHA_CONFIG["timeout"]) as response:
                if response.status != 200:
                    return f"æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status}"
                    
                data = await response.json()
                
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if "data" not in data:
                    return "æœªæ‰¾åˆ°æœç´¢ç»“æœ"
                
                if "webPages" not in data["data"]:
                    return "æœªæ‰¾åˆ°æœç´¢ç»“æœ"
                
                if "value" not in data["data"]["webPages"]:
                    return "æœªæ‰¾åˆ°æœç´¢ç»“æœ"
                    
                results = data["data"]["webPages"]["value"]
                if not results:
                    return "æœªæ‰¾åˆ°æœç´¢ç»“æœ"
                    
                # å¤„ç†æœç´¢ç»“æœ
                formatted_results = []
                for i, result in enumerate(results[:5], 1):
                    title = result.get('name', f'æœç´¢ç»“æœ {i}')
                    content = result.get('snippet', result.get('summary', 'æš‚æ— å†…å®¹æ‘˜è¦'))
                    
                    # æ›´å®‰å…¨åœ°è·å–URL
                    result_url = ''
                    if 'url' in result:
                        url_value = result['url']
                        if isinstance(url_value, str):
                            result_url = url_value
                        elif isinstance(url_value, dict) and 'href' in url_value:
                            result_url = url_value['href']
                    
                    # æ ¼å¼åŒ–è¾“å‡ºï¼Œç›´æ¥ç”ŸæˆHTMLé“¾æ¥ä»¥ä¾¿åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€
                    if result_url and result_url.startswith('http'):
                        formatted_results.append(f"{i}. **{title}**\n{content}\né“¾æ¥ï¼š<a href=\"{result_url}\" target=\"_blank\" rel=\"noopener noreferrer\">{result_url}</a>\n")
                    else:
                        formatted_results.append(f"{i}. **{title}**\n{content}\n")
                
                final_result = "ç›¸å…³æœç´¢ç»“æœï¼š\n" + "\n".join(formatted_results)
                return final_result
                
    except asyncio.TimeoutError:
        return "ç½‘ç»œæœç´¢è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
    except Exception as e:
        return f"ç½‘ç»œæœç´¢å¤±è´¥ï¼š{str(e)}"

async def initialize_mcp_tools():
    """åˆå§‹åŒ–MCPå·¥å…·åˆ—è¡¨"""
    global mcp_tools_cache
    try:
        conn = sqlite3.connect('history_messages.db')
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰MCPæœåŠ¡å™¨å’Œå·¥å…·
        cursor.execute('''
            SELECT s.id as server_id, s.name as server_name, s.url as server_url,
                   t.id as tool_id, t.name as tool_name, t.description as tool_description,
                   t.input_schema
            FROM mcp_servers s
            LEFT JOIN mcp_tools t ON s.id = t.server_id
            WHERE s.id IS NOT NULL
        ''')
        
        results = cursor.fetchall()
        conn.close()
        
        # ç»„ç»‡å·¥å…·æ•°æ®
        mcp_tools_cache = {}
        for row in results:
            server_id = row['server_id']
            if server_id not in mcp_tools_cache:
                mcp_tools_cache[server_id] = {
                    'server_name': row['server_name'],
                    'server_url': row['server_url'],
                    'tools': []
                }
            
            if row['tool_id']:  # å¦‚æœæœ‰å·¥å…·
                # è§£æinput_schema JSONå­—ç¬¦ä¸²
                try:
                    input_schema = json.loads(row['input_schema']) if row['input_schema'] else {}
                except (json.JSONDecodeError, TypeError):
                    input_schema = row['input_schema'] if row['input_schema'] else {}
                
                mcp_tools_cache[server_id]['tools'].append({
                    'id': row['tool_id'],
                    'name': row['tool_name'],
                    'description': row['tool_description'],
                    'input_schema': input_schema
                })
        
        print(f"å·²åˆå§‹åŒ– {len(mcp_tools_cache)} ä¸ªMCPæœåŠ¡å™¨çš„å·¥å…·")
        
    except Exception as e:
        print(f"åˆå§‹åŒ–MCPå·¥å…·å¤±è´¥: {str(e)}")
        mcp_tools_cache = {}

async def call_mcp_service(server_url: str, tool_name: str, parameters: dict) -> str:
    """è°ƒç”¨MCPæœåŠ¡"""
    try:
        print(f"ğŸ”§ è°ƒç”¨MCPæœåŠ¡: {server_url}, å·¥å…·: {tool_name}, å‚æ•°: {parameters}")
        
        from fastmcp import Client
        from fastmcp.client.transports import SSETransport
        
        async with Client(SSETransport(server_url)) as client:
            result = await client.call_tool(tool_name, parameters)
            print(f"âœ… FastMCPè°ƒç”¨æˆåŠŸ: {result}")
            return str(result)
            
    except Exception as e:
        print(f"âŒ FastMCPè°ƒç”¨å¤±è´¥: {str(e)}")
        return f"è°ƒç”¨MCPæœåŠ¡å¤±è´¥: {str(e)}"

async def generate_mcp_answer(client, query, conversation_context="", model_name="glm-4-plus"):
    """ä½¿ç”¨MCPæœåŠ¡ç”Ÿæˆç­”æ¡ˆ"""
    global mcp_tools_cache
    
    print(f"ğŸ¤– å¼€å§‹MCPç­”æ¡ˆç”Ÿæˆï¼ŒæŸ¥è¯¢: {query}")
    print(f"ğŸ“Š MCPå·¥å…·ç¼“å­˜çŠ¶æ€: {len(mcp_tools_cache) if mcp_tools_cache else 0} ä¸ªæœåŠ¡å™¨")
    
    # ç®€åŒ–çš„ç¼“å­˜å†…å®¹è°ƒè¯•è¾“å‡º
    if mcp_tools_cache:
        for server_id, server_info in mcp_tools_cache.items():
            server_name = server_info.get('server_name', 'Unknown')
            tool_count = len(server_info.get('tools', []))
            print(f"  ğŸ“¡ {server_name}: {tool_count} ä¸ªå·¥å…·")
    else:
        print("âŒ MCPå·¥å…·ç¼“å­˜ä¸ºç©º!")
    
    # æ„å»ºå·¥å…·æè¿°
    tool_descriptions = []
    for server_id, server_info in mcp_tools_cache.items():
        server_name = server_info['server_name']
        server_url = server_info['server_url']
        for tool in server_info['tools']:
            tool_desc = f"æœåŠ¡å™¨: {server_name} ({server_url})\nå·¥å…·åç§°: {tool['name']}\næè¿°: {tool['description']}\nå‚æ•°ç»“æ„: {tool['input_schema']}"
            tool_descriptions.append(tool_desc)
    
    tools_text = "\n\n".join(tool_descriptions) if tool_descriptions else "æš‚æ— å¯ç”¨å·¥å…·"
    print(f"ğŸ”§ å·¥å…·æè¿°å‡†å¤‡å®Œæˆï¼Œå…± {len(tool_descriptions)} ä¸ªå·¥å…·")
    
    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡æŸ¥è¯¢
    # å¦‚æœæœ‰å†å²å¯¹è¯ï¼Œéœ€è¦ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾
    contextual_query = query
    if conversation_context.strip():
        # è®©å¤§æ¨¡å‹å…ˆç†è§£å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„æŸ¥è¯¢
        context_understanding_prompt = f"""åŸºäºä»¥ä¸‹å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜ï¼Œè¯·ç†è§£ç”¨æˆ·çš„çœŸå®æŸ¥è¯¢æ„å›¾ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®çš„æŸ¥è¯¢æè¿°ã€‚

å†å²å¯¹è¯ï¼š
{conversation_context}

å½“å‰é—®é¢˜ï¼š{query}

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æŸ¥è¯¢æè¿°ï¼Œèƒ½å¤Ÿå‡†ç¡®è¡¨è¾¾ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„å†…å®¹ï¼š"""
        
        try:
            context_response = await client.chat.completions.create(
                model=MODEL_CONFIG[model_name]["model"],
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šä¸‹æ–‡ç†è§£åŠ©æ‰‹ã€‚è¯·æ ¹æ®å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®çš„æŸ¥è¯¢æè¿°ã€‚"
                    },
                    {
                        "role": "user",
                        "content": context_understanding_prompt
                    }
                ],
                temperature=0.1,
                max_tokens=1000
            )
            contextual_query = context_response.choices[0].message.content.strip()
            print(f"ğŸ” ä¸Šä¸‹æ–‡ç†è§£åçš„æŸ¥è¯¢: {contextual_query}")
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡ç†è§£å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {str(e)}")
            contextual_query = query
    
    # ç¬¬ä¸€æ­¥ï¼šè®©å¤§æ¨¡å‹é€‰æ‹©å·¥å…·
    tool_selection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œçµæ´»é€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œæ“ä½œã€‚  
ä½ çš„ç›®æ ‡æ˜¯é«˜æ•ˆã€å‡†ç¡®åœ°è§£å†³ç”¨æˆ·çš„éœ€æ±‚ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™æ‰§è¡Œï¼š

1. å·¥å…·è°ƒç”¨è§„èŒƒ  
+- å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·ä¸¥æ ¼è¿”å›å¦‚ä¸‹ JSON æ ¼å¼ï¼ˆä¸è¦åŒ…å«å¤šä½™å†…å®¹ï¼‰ï¼š
```json
{{
  "server_url": "server_url",
  "tool_name": "tool_name",
  "parameters": {{
    "param1": "value1",
    "param2": "value2"
  }}
}}
```
+- åªè¿”å› JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ã€‚
+- é‡è¦ï¼šåœ¨æ„å»ºå·¥å…·å‚æ•°æ—¶ï¼Œè¯·ä½¿ç”¨å®Œæ•´çš„ä¸Šä¸‹æ–‡æŸ¥è¯¢å†…å®¹ï¼Œè€Œä¸æ˜¯ç®€çŸ­çš„ç”¨æˆ·è¾“å…¥ã€‚

2. ç›´æ¥å›ç­”è§„èŒƒ  
+- å¦‚æœä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼Œç›´æ¥è¿”å›æœ€ç»ˆçš„ä¸­æ–‡å›ç­”å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½• JSON æˆ–æ ¼å¼åŒ–å†…å®¹ã€‚

3. å·¥å…·é€‰æ‹©åŸåˆ™  
+- ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜ï¼Œä¼˜å…ˆé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚
+- å·¥å…·å‚æ•°è¦å‚ç…§å·¥å…·æè¿°ï¼Œä½¿ç”¨å®Œæ•´çš„æŸ¥è¯¢å†…å®¹ã€‚
+- å¦‚æœç”¨æˆ·é—®é¢˜ä¸æ˜ç¡®ï¼Œå¯ä»¥é€‚å½“è¿½é—®è¡¥å……ä¿¡æ¯ï¼Œä½†è¦ç®€æ´ã€‚

4. è§’è‰²è®¾å®š  
+- ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€è€å¿ƒã€å–„äºåˆ†æçš„åŠ©æ‰‹ï¼Œå–„äºç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šå¤æ‚é—®é¢˜ã€‚

5. å…¶ä»–æ³¨æ„äº‹é¡¹  
+- ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„å·¥å…·æˆ–å‚æ•°ã€‚
+- ä¸è¦è¾“å‡ºä»»ä½•ä¸å·¥å…·è°ƒç”¨æ— å…³çš„å†…å®¹ã€‚
+- ä¿æŒè¾“å‡ºå†…å®¹çš„æ ¼å¼ä¸¥æ ¼ç¬¦åˆè¦æ±‚ã€‚

å†å²å¯¹è¯ï¼š
+{conversation_context}

ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{query}

å®Œæ•´ä¸Šä¸‹æ–‡æŸ¥è¯¢ï¼š{contextual_query}

å¯ç”¨å·¥å…·åˆ—è¡¨åŠæè¿°ï¼š
+{tools_text}

æ³¨æ„ï¼šåœ¨è°ƒç”¨å·¥å…·æ—¶ï¼Œè¯·ä½¿ç”¨"å®Œæ•´ä¸Šä¸‹æ–‡æŸ¥è¯¢"çš„å†…å®¹ä½œä¸ºæŸ¥è¯¢å‚æ•°ï¼Œè¿™æ ·èƒ½ç¡®ä¿å·¥å…·èƒ½å¤Ÿå‡†ç¡®ç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ã€‚
"""

    try:
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šå·¥å…·é€‰æ‹©
        response = await client.chat.completions.create(
            model=MODEL_CONFIG[model_name]["model"],
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·é€‰æ‹©åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œæˆ–ç›´æ¥å›ç­”é—®é¢˜ã€‚"
                },
                {
                    "role": "user",
                    "content": tool_selection_prompt
                }
            ],
            temperature=0.1,
            max_tokens=4096
        )
        
        tool_response = response.choices[0].message.content.strip()
        print(f"ğŸ§  å¤§æ¨¡å‹å·¥å…·é€‰æ‹©å“åº”: {tool_response}")
        
        # å°è¯•è§£æJSON
        try:
            # å¤„ç†å¯èƒ½åŒ…å«markdownä»£ç å—çš„å“åº”
            json_content = tool_response.strip()
            
            # å¦‚æœåŒ…å«```jsonä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSONå†…å®¹
            if json_content.startswith('```json') and json_content.endswith('```'):
                json_content = json_content[7:-3].strip()  # ç§»é™¤```jsonå’Œ```
            elif json_content.startswith('```') and json_content.endswith('```'):
                json_content = json_content[3:-3].strip()  # ç§»é™¤```å’Œ```
            
            tool_call = json.loads(json_content)
            print(f"âœ… æˆåŠŸè§£æå·¥å…·è°ƒç”¨JSON: {tool_call}")
            if "server_url" in tool_call and "tool_name" in tool_call and "parameters" in tool_call:
                print(f"ğŸ¯ å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call['tool_name']}")
                
                # æ ¹æ®æœåŠ¡å™¨åç§°æŸ¥æ‰¾å®é™…çš„URL
                server_name = tool_call["server_url"]
                actual_server_url = None
                for server_id, server_info in mcp_tools_cache.items():
                    if server_info['server_name'] == server_name:
                        actual_server_url = server_info['server_url']
                        break
                
                if actual_server_url is None:
                    print(f"âŒ æœªæ‰¾åˆ°æœåŠ¡å™¨ {server_name} çš„URLé…ç½®")
                    return f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æœåŠ¡å™¨ {server_name} çš„é…ç½®"
                
                print(f"ğŸ”— æœåŠ¡å™¨åç§°: {server_name}, å®é™…URL: {actual_server_url}")
                
                # è°ƒç”¨MCPæœåŠ¡
                mcp_result = await call_mcp_service(
                    actual_server_url,
                    tool_call["tool_name"],
                    tool_call["parameters"]
                )
                print(f"ğŸ“‹ MCPè°ƒç”¨ç»“æœé•¿åº¦: {len(str(mcp_result))} å­—ç¬¦")
                
                # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šåŸºäºMCPç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                final_prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

å†å²å¯¹è¯ï¼š
+{conversation_context}

MCPæœåŠ¡è°ƒç”¨ç»“æœï¼š
+{mcp_result}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
                
                final_response = await client.chat.completions.create(
                    model=MODEL_CONFIG[model_name]["model"],
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚è¯·ç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                        },
                        {
                            "role": "user",
                            "content": final_prompt
                        }
                    ],
                    temperature=0.3,
                    stream=True,
                    max_tokens=8192
                )
                
                return final_response
                
        except json.JSONDecodeError as json_error:
            # å¦‚æœä¸æ˜¯JSONï¼Œè¯´æ˜æ˜¯ç›´æ¥å›ç­”
            print(f"ğŸ“ å¤§æ¨¡å‹é€‰æ‹©ç›´æ¥å›ç­”æ¨¡å¼ï¼ŒJSONè§£æå¤±è´¥: {str(json_error)}")
            print(f"ğŸ“„ åŸå§‹å“åº”å†…å®¹: {tool_response[:200]}...")
        
        # ç›´æ¥è¿”å›æµå¼å“åº”
        direct_response = await client.chat.completions.create(
            model=MODEL_CONFIG[model_name]["model"],
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚è¯·ç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                },
                {
                    "role": "user",
                    "content": f"å†å²å¯¹è¯ï¼š\n{conversation_context}\n\nå½“å‰é—®é¢˜ï¼š{query}\n\nè¯·å›ç­”ï¼š"
                }
            ],
            temperature=0.3,
            stream=True,
            max_tokens=8192
        )
        
        return direct_response
        
    except Exception as e:
        raise

async def generate_answer(client, query, context, conversation_context="", model_name="glm-4-plus", web_search_context=""):
    """ç”Ÿæˆç­”æ¡ˆ"""
    # æ ¹æ®æ˜¯å¦æœ‰ç½‘ç»œæœç´¢ç»“æœåˆ¤æ–­ä½¿ç”¨åœºæ™¯
    is_web_search_mode = bool(web_search_context and not context)
    
    if is_web_search_mode:
        # è”ç½‘æœç´¢æ¨¡å¼ï¼šåªä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœå’Œå†å²å¯¹è¯
        if conversation_context:
            prompt = f"""åŸºäºä»¥ä¸‹å†å²å¯¹è¯å’Œç½‘ç»œæœç´¢ç»“æœå›ç­”é—®é¢˜ï¼š

å†å²å¯¹è¯ï¼š
+{conversation_context}

ç½‘ç»œæœç´¢ç»“æœï¼š
+{web_search_context}

å½“å‰é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å›ç­”ï¼š"""
        else:
            prompt = f"""åŸºäºä»¥ä¸‹ç½‘ç»œæœç´¢ç»“æœå›ç­”é—®é¢˜ï¼š

ç½‘ç»œæœç´¢ç»“æœï¼š
+{web_search_context}

é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šç½‘ç»œæœç´¢ä¿¡æ¯ï¼Œç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å›ç­”ï¼š"""
    else:
        # æœ¬åœ°æ–‡æ¡£æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°æ–‡æ¡£å’Œå†å²å¯¹è¯
        if conversation_context:
            prompt = f"""åŸºäºä»¥ä¸‹å†å²å¯¹è¯å’Œç›¸å…³ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

å†å²å¯¹è¯ï¼š
+{conversation_context}

ç›¸å…³ä¿¡æ¯ï¼š
+{context}

å½“å‰é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å›ç­”ï¼š"""
        else:
            prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

+{context}

é—®é¢˜ï¼š{query}

è¯·ç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å›ç­”ï¼š"""
    
    try:
        # ä¼˜åŒ–æ¨¡å‹å‚æ•°
        response = await client.chat.completions.create(
            model=MODEL_CONFIG[model_name]["model"],
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚è¯·ç”¨å‡†ç¡®ã€æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡æœ‰é“¾æ¥ï¼Œè¯·åœ¨å›ç­”ä¸­æä¾›é“¾æ¥ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œä½¿è¾“å‡ºæ›´ç¡®å®š
            top_p=0.8,        # è°ƒæ•´é‡‡æ ·èŒƒå›´
            frequency_penalty=0.5,  # æ·»åŠ é¢‘ç‡æƒ©ç½šï¼Œé¿å…é‡å¤
            presence_penalty=0.5,   # æ·»åŠ å­˜åœ¨æƒ©ç½šï¼Œé¼“åŠ±å¤šæ ·æ€§
            stream=True,      # å¯ç”¨æµå¼è¾“å‡º
            max_tokens=8192,  # è®¾ç½®ä¸ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§å€¼
            stream_options={"include_usage": False}  # å‡å°‘å“åº”æ•°æ®é‡
        )
        
        return response
        
    except Exception as e:
        raise

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œç´¢å¼•
model = None
index = None
all_chunks = None
client = None
doc_sources = None
chunks_to_document = None  # æ·»åŠ  chunks_to_document åˆ°å…¨å±€å˜é‡
mcp_tools_cache = {}  # ç¼“å­˜MCPå·¥å…·åˆ—è¡¨

async def initialize_system():
    """åˆå§‹åŒ–ç³»ç»Ÿï¼ŒåŠ è½½æ¨¡å‹å’Œæ–‡æ¡£"""
    global model, index, all_chunks, client, doc_sources, chunks_to_document
    
    # åŠ è½½æ¨¡å‹
    model = load_model()
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜ å°„æ–‡ä»¶ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥åŠ è½½
    chunks_map_path = 'chunks_mapping.npy'
    index_path = 'faiss_index.faiss'
    
    if os.path.exists(chunks_map_path) and os.path.exists(index_path):
        print("å‘ç°å·²å­˜åœ¨çš„ç´¢å¼•å’Œæ˜ å°„æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
        
        # åŠ è½½æ˜ å°„æ–‡ä»¶
        try:
            mapping_data = np.load(chunks_map_path, allow_pickle=True).item()
            chunks_to_document = mapping_data.get('chunks_to_document', {})
            document_to_chunks = mapping_data.get('document_to_chunks', {})
            
            # åŠ è½½ç´¢å¼•
            index = faiss.read_index(index_path)
            
            # é‡å»ºall_chunkså’Œdoc_sources
            documents, doc_sources = load_docs()
            if not documents:
                print("é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ç›®å½•å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆæ–‡ä»¶")
                return False
            
            all_chunks = []
            for i, (doc, source) in enumerate(zip(documents, doc_sources)):
                if i in document_to_chunks:
                    # è·å–æ–‡ä»¶å
                    file_name = os.path.basename(source)
                    # æ£€æŸ¥æ˜¯å¦ä¸ºExcelæ–‡ä»¶
                    is_excel = source.lower().endswith(('.xlsx', '.xls'))
                    chunks = chunk_document(doc, is_excel=is_excel, file_name=file_name)
                    all_chunks.extend(chunks)
                else:
                    print(f"è­¦å‘Š: æ–‡æ¡£ {source} åœ¨æ˜ å°„ä¸­æœªæ‰¾åˆ°")
            
            print(f"æˆåŠŸåŠ è½½å·²æœ‰ç´¢å¼•ï¼ŒåŒ…å« {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
            
        except Exception as e:
            print(f"åŠ è½½å·²æœ‰ç´¢å¼•å¤±è´¥: {str(e)}ï¼Œå°†é‡æ–°å¤„ç†æ–‡æ¡£")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ é™¤æŸåçš„æ–‡ä»¶å¹¶é‡æ–°å¤„ç†
            if os.path.exists(chunks_map_path):
                os.remove(chunks_map_path)
            if os.path.exists(index_path):
                os.remove(index_path)
            return await initialize_system()  # é€’å½’è°ƒç”¨é‡æ–°å¤„ç†
    else:
        print("æœªå‘ç°ç´¢å¼•æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†æ–‡æ¡£...")
        
        # åŠ è½½æ–‡æ¡£
        documents, doc_sources = load_docs()
        
        if not documents:
            print("é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ç›®å½•å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆæ–‡ä»¶")
            return False

        # å¤„ç†æ–‡æ¡£åˆ†å—
        document_to_chunks = {}
        chunks_to_document = {}
        all_chunks = []
        
        # åˆ›å»º chunks ç›®å½•
        chunks_dir = Path('chunks')
        chunks_dir.mkdir(exist_ok=True)
        
        print("å¼€å§‹å¤„ç†æ–‡æ¡£åˆ†å—")
        for i, (doc, source) in enumerate(zip(documents, doc_sources)):
            # è·å–æ–‡ä»¶å
            file_name = os.path.basename(source)
            # æ£€æŸ¥æ˜¯å¦ä¸ºExcelæ–‡ä»¶
            is_excel = source.lower().endswith(('.xlsx', '.xls'))
            chunks = chunk_document(doc, is_excel=is_excel, file_name=file_name)
            document_to_chunks[i] = chunks
            
            # ä¿å­˜æ–‡æ¡£å—åˆ° JSON æ–‡ä»¶
            chunks_data = {
                "file_name": file_name,
                "file_path": source,
                "total_chunks": len(chunks),
                "chunks": [
                    {
                        "chunk_id": j,
                        "content": chunk,
                        "length": len(chunk)
                    }
                    for j, chunk in enumerate(chunks)
                ]
            }
            
            # å°†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œä½œä¸º JSON æ–‡ä»¶å
            safe_filename = re.sub(r'[^\w\-_.]', '_', file_name)
            json_path = chunks_dir / f"{safe_filename}.json"
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(chunks_data, f, ensure_ascii=False, indent=2)
            
            for chunk in chunks:
                chunks_to_document[len(all_chunks)] = i
                all_chunks.append(chunk)
        
        if not all_chunks:
            print("é”™è¯¯: æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£å—")
            return False
            
        print(f"æˆåŠŸç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")

        # åˆ›å»ºæ–°çš„ç´¢å¼•
        print("åˆ›å»ºæ–°çš„ç´¢å¼•")
        embeddings = get_embeddings(model, all_chunks)
        
        # ä½¿ç”¨IVFç´¢å¼•ç±»å‹ï¼Œæé«˜æœç´¢æ•ˆç‡
        dimension = embeddings.shape[1]
        nlist = min(100, len(all_chunks) // 10)  # èšç±»ä¸­å¿ƒæ•°é‡
        quantizer = faiss.IndexFlatL2(dimension)
        index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
        
        # è®­ç»ƒç´¢å¼•
        if not index.is_trained and len(all_chunks) > nlist:
            print("è®­ç»ƒç´¢å¼•...")
            index.train(embeddings)
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        index.add(embeddings)
        
        # ä¿å­˜ç´¢å¼•å’Œæ˜ å°„
        print(f"ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°: {index_path}")
        faiss.write_index(index, index_path)
        np.save(chunks_map_path, {
            'document_to_chunks': document_to_chunks,
            'chunks_to_document': chunks_to_document
        })

    # åˆå§‹åŒ–é»˜è®¤çš„OpenAIå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨GLM-4-PLUSï¼‰
    client = create_optimized_client("glm-4-plus")
    
    # åˆå§‹åŒ–MCPå·¥å…·
    await initialize_mcp_tools()
    
    return True

def reprocess_documents():
    """é‡æ–°å¤„ç†æ‰€æœ‰æ–‡æ¡£"""
    global model, index, all_chunks, doc_sources, chunks_to_document
    
    print("å¼€å§‹é‡æ–°å¤„ç†æ–‡æ¡£")
    
    # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
    if model is None:
        model = load_model()
        if model is None:
            print("é”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹")
            return False
    
    # åŠ è½½æ–‡æ¡£
    documents, doc_sources = load_docs()
    
    if not documents:
        print("é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ç›®å½•å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆæ–‡ä»¶")
        return False
    
    # å¤„ç†æ–‡æ¡£åˆ†å—
    document_to_chunks = {}
    chunks_to_document = {}
    all_chunks = []
    
    # åˆ›å»º chunks ç›®å½•
    chunks_dir = Path('chunks')
    chunks_dir.mkdir(exist_ok=True)
    
    print("å¼€å§‹å¤„ç†æ–‡æ¡£åˆ†å—")
    for i, (doc, source) in enumerate(zip(documents, doc_sources)):
        # è·å–æ–‡ä»¶å
        file_name = os.path.basename(source)
        # æ£€æŸ¥æ˜¯å¦ä¸ºExcelæ–‡ä»¶
        is_excel = source.lower().endswith(('.xlsx', '.xls'))
        chunks = chunk_document(doc, is_excel=is_excel, file_name=file_name)
        document_to_chunks[i] = chunks
        
        # ä¿å­˜æ–‡æ¡£å—åˆ° JSON æ–‡ä»¶
        chunks_data = {
            "file_name": file_name,
            "file_path": source,
            "total_chunks": len(chunks),
            "chunks": [
                {
                    "chunk_id": j,
                    "content": chunk,
                    "length": len(chunk)
                }
                for j, chunk in enumerate(chunks)
            ]
        }
        
        # å°†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œä½œä¸º JSON æ–‡ä»¶å
        safe_filename = re.sub(r'[^\w\-_.]', '_', file_name)
        json_path = chunks_dir / f"{safe_filename}.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        for chunk in chunks:
            chunks_to_document[len(all_chunks)] = i
            all_chunks.append(chunk)
    
    if not all_chunks:
        print("é”™è¯¯: æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£å—")
        return False
        
    print(f"æˆåŠŸç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    # åˆ›å»ºæ–°çš„ç´¢å¼•
    print("åˆ›å»ºæ–°çš„ç´¢å¼•")
    embeddings = get_embeddings(model, all_chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    
    # ä¿å­˜ç´¢å¼•å’Œæ˜ å°„
    index_path = 'faiss_index.faiss'
    chunks_map_path = 'chunks_mapping.npy'
    print(f"ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°: {index_path}")
    faiss.write_index(index, index_path)
    np.save(chunks_map_path, {
        'document_to_chunks': document_to_chunks,
        'chunks_to_document': chunks_to_document
    })
    
    return True

def add_document_to_index(model, index, all_chunks, chunks_to_document, doc_path, doc_source_idx):
    print("å¼€å§‹å¤„ç†æ–°ä¸Šä¼ çš„æ–‡æ¡£")
    # åŠ è½½æ–‡æ¡£å†…å®¹
    loader = DocumentLoader()
    doc_info = loader.load_document(doc_path)
    file_name = os.path.basename(doc_path)
    is_excel = file_name.lower().endswith(('.xlsx', '.xls'))
    chunks = chunk_document(doc_info['content'], is_excel=is_excel, file_name=file_name)
    if not chunks:
        print("æœªç”Ÿæˆä»»ä½•æ–‡æ¡£å—ï¼Œè·³è¿‡")
        return False
    # è®¡ç®—æ–°å—çš„å‘é‡
    new_embeddings = get_embeddings(model, chunks)
    # æ›´æ–°ç´¢å¼•
    index.add(new_embeddings)
    # æ›´æ–°all_chunkså’Œchunks_to_document
    start_idx = len(all_chunks)
    all_chunks.extend(chunks)
    for i in range(len(chunks)):
        chunks_to_document[start_idx + i] = doc_source_idx
    # ä¿å­˜ç´¢å¼•å’Œæ˜ å°„
    faiss.write_index(index, 'faiss_index.faiss')
    # éœ€è¦é‡æ–°æ„å»ºdocument_to_chunksæ˜ å°„
    document_to_chunks = {}
    for chunk_idx, doc_idx in chunks_to_document.items():
        if doc_idx not in document_to_chunks:
            document_to_chunks[doc_idx] = []
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®chunkå†…å®¹é‡å»º
    
    np.save('chunks_mapping.npy', {
        'chunks_to_document': chunks_to_document,
        'document_to_chunks': document_to_chunks
    })
    print("å¢é‡å…¥åº“å®Œæˆ")
    return True

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    try:
        if await initialize_system():
            print("ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        else:
            print("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œä½†åº”ç”¨ä»å¯å¯åŠ¨")
    except Exception as e:
        print(f"ç³»ç»Ÿåˆå§‹åŒ–å‡ºé”™: {str(e)}ï¼Œä½†åº”ç”¨ä»å¯å¯åŠ¨")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    try:
        await db.close()
        # ä¿å­˜ç¼“å­˜
        embedding_cache.save_cache()
        vector_cache.save_cache()
    except Exception as e:
        print(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {str(e)}")

# ä¿®æ”¹ FastAPI åº”ç”¨åˆ›å»º
app = FastAPI(lifespan=lifespan)

# æ·»åŠ MCPè·¯ç”±
app.include_router(mcp_router)


# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ å¤„ç† Chrome å¼€å‘è€…å·¥å…·è¯·æ±‚çš„è·¯ç”±
@app.get("/.well-known/appspecific/com.chrome.devtools.json")
async def chrome_devtools():
    return JSONResponse(
        content={"status": "ok"},
        status_code=200
    )

# åˆ›å»ºæ–‡æ¡£åŠ è½½å™¨å®ä¾‹
document_loader = DocumentLoader()

# åˆ›å»ºä¸Šä¼ æ–‡ä»¶ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# è®¾ç½®æ¨¡æ¿ç›®å½•
templates = Jinja2Templates(directory=os.path.join(BASE_DIR, "templates"))

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """
    è¿”å›ä¸»é¡µ
    åŠŸèƒ½ï¼šåŠ è½½å¹¶æ˜¾ç¤ºèŠå¤©ç•Œé¢ï¼ŒåŒ…æ‹¬å†å²å¯¹è¯åˆ—è¡¨
    å‚æ•°ï¼š
        - request: è¯·æ±‚å¯¹è±¡
    è¿”å›ï¼š
        - HTMLé¡µé¢ï¼ŒåŒ…å«å†å²å¯¹è¯åˆ—è¡¨å’ŒèŠå¤©ç•Œé¢
    """
    try:
        conversations = await db.get_conversations()
        history_items = [
            {
                "id": conv[0],
                "title": conv[1],
                "timestamp": conv[2]
            }
            for conv in conversations
        ]
        
        return templates.TemplateResponse(
            "index.html",
            {
                "request": request,
                "historyItems": history_items,
                "chatMessages": [],
                "files": []
            }
        )
    except Exception as e:
        return templates.TemplateResponse(
            "index.html",
            {
                "request": request,
                "historyItems": [],
                "chatMessages": [],
                "files": []
            }
        )

@app.get("/files")
async def get_files():
    """
    è·å–å¯ç”¨çš„æ–‡ä»¶åˆ—è¡¨
    åŠŸèƒ½ï¼šè¿”å›uploadsç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ååˆ—è¡¨
    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š.txt, .pdf, .docx, .md, .xlsx, .xls
    è¿”å›ï¼š
        - æ–‡ä»¶ååˆ—è¡¨
    """
    try:
        files = []
        for ext in ['.txt', '.pdf', '.docx', '.md', '.xlsx', '.xls']:
            files.extend(glob.glob(os.path.join('uploads', f'*{ext}')))
        return [os.path.basename(f) for f in files]
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"}
        )

def log_performance(step_name, start_time):
    """è®°å½•æ€§èƒ½æ—¥å¿—"""
    end_time = time.time()
    duration = end_time - start_time
    print(f"[æ€§èƒ½æ—¥å¿—] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {step_name}: {duration:.2f}ç§’")

@app.get("/query/{question}")
async def query_endpoint(
    question: str,
    file: str = Query(None, description="æŒ‡å®šè¦æŸ¥è¯¢çš„æ–‡ä»¶å"),
    conversation_id: str = Query(None, description="ä¼šè¯ID"),
    model_name: str = Query("glm-4-plus", description="é€‰æ‹©çš„å¤§æ¨¡å‹åç§°"),
    web_search_enabled: bool = Query(False, description="æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢"),
    mcp_service_enabled: bool = Query(False, description="æ˜¯å¦å¯ç”¨MCPæœåŠ¡"),
    request: Request = None
):
    # æ€§èƒ½è°ƒè¯•ï¼šè®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    print(f"ğŸš€ [æ€§èƒ½è°ƒè¯•] è¯·æ±‚å¼€å§‹: {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")
    
    global model, index, all_chunks, client, doc_sources, chunks_to_document
    
    # URLè§£ç é—®é¢˜
    question = urllib.parse.unquote(question)
    print(f"ğŸ“ [æ€§èƒ½è°ƒè¯•] é—®é¢˜è§£ç å®Œæˆ: {time.time() - total_start_time:.3f}s")
    
    # ç³»ç»Ÿåˆå§‹åŒ–æ£€æŸ¥
    init_start = time.time()
    if not await quick_system_check():
        return JSONResponse(
            status_code=500,
            content={"error": "ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"}
        )
    print(f"ğŸ”§ [æ€§èƒ½è°ƒè¯•] ç³»ç»Ÿåˆå§‹åŒ–æ£€æŸ¥: {time.time() - init_start:.3f}s")
    
    try:
        # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æœ‰æ•ˆ
        model_check_start = time.time()
        if model_name not in MODEL_CONFIG:
            return JSONResponse(
                status_code=400,
                content={"error": f"ä¸æ”¯æŒçš„æ¨¡å‹åç§°: {model_name}"}
            )
        
        # æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        client = create_optimized_client(model_name)
        print(f"ğŸ¤– [æ€§èƒ½è°ƒè¯•] æ¨¡å‹é…ç½®å’Œå®¢æˆ·ç«¯åˆå§‹åŒ–: {time.time() - model_check_start:.3f}s")
        
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context_start = time.time()
        conversation_context = ""
        if conversation_id:
            # è·å–å†å²å¯¹è¯ä¸Šä¸‹æ–‡
            conversation_context, is_overflow = await db.get_conversation_context(conversation_id)
            if is_overflow:
                return JSONResponse(
                    status_code=400,
                    content={"error": "è¯¥å¯¹è¯å†…å®¹è¿‡å¤šï¼Œè¯·å¼€å¯æ–°å¯¹è¯"}
                )
        else:
            # åˆ›å»ºæ–°å¯¹è¯
            conversation_id = await db.create_conversation(question)
        print(f"ğŸ’¬ [æ€§èƒ½è°ƒè¯•] å¯¹è¯ä¸Šä¸‹æ–‡è·å–: {time.time() - context_start:.3f}s")
        
        # ä¿å­˜ç”¨æˆ·é—®é¢˜
        save_msg_start = time.time()
        await db.add_message(conversation_id, question, "user")
        print(f"ğŸ’¾ [æ€§èƒ½è°ƒè¯•] ä¿å­˜ç”¨æˆ·æ¶ˆæ¯: {time.time() - save_msg_start:.3f}s")
        
        # æ ¹æ®å¯ç”¨çš„æœåŠ¡ç±»å‹å†³å®šæ•°æ®æº
        service_start = time.time()
        if mcp_service_enabled:
            print(f"ğŸ”§ [æ€§èƒ½è°ƒè¯•] é€‰æ‹©MCPæœåŠ¡æ¨¡å¼")
            # å¯ç”¨MCPæœåŠ¡æ—¶ï¼Œä½¿ç”¨MCPæœåŠ¡å¤„ç†
            response = await generate_mcp_answer(client, question, conversation_context, model_name)
            context = ""  # ä¸ä½¿ç”¨æœ¬åœ°æ–‡æ¡£ä¸Šä¸‹æ–‡
            web_search_context = ""  # ä¸ä½¿ç”¨ç½‘ç»œæœç´¢
        elif web_search_enabled:
            print(f"ğŸŒ [æ€§èƒ½è°ƒè¯•] é€‰æ‹©è”ç½‘æœç´¢æ¨¡å¼")
            # å¯ç”¨è”ç½‘æœç´¢æ—¶ï¼Œåªä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœï¼Œä¸æ£€ç´¢æœ¬åœ°æ–‡æ¡£
            web_search_start = time.time()
            web_search_context = await web_search(question)
            print(f"ğŸ” [æ€§èƒ½è°ƒè¯•] ç½‘ç»œæœç´¢å®Œæˆ: {time.time() - web_search_start:.3f}s")
            context = ""  # ä¸ä½¿ç”¨æœ¬åœ°æ–‡æ¡£ä¸Šä¸‹æ–‡
            
            generate_start = time.time()
            response = await generate_answer(client, question, context, conversation_context, model_name, web_search_context)
            print(f"ğŸ¤– [æ€§èƒ½è°ƒè¯•] ç”Ÿæˆç­”æ¡ˆè°ƒç”¨å®Œæˆ: {time.time() - generate_start:.3f}s")
        else:
            print(f"ğŸ“š [æ€§èƒ½è°ƒè¯•] é€‰æ‹©æœ¬åœ°æ–‡æ¡£æ¨¡å¼")
            # æœªå¯ç”¨è”ç½‘æœç´¢æ—¶ï¼Œä½¿ç”¨æœ¬åœ°æ–‡æ¡£æ£€ç´¢
            doc_search_start = time.time()
            context, indices = get_related_documents(model, index, question, all_chunks)
            print(f"ğŸ“– [æ€§èƒ½è°ƒè¯•] æ–‡æ¡£æ£€ç´¢å®Œæˆ: {time.time() - doc_search_start:.3f}s")
            
            if not context:
                return JSONResponse(
                    status_code=404,
                    content={"error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"}
                )
            
            # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ï¼Œè¿‡æ»¤ç›¸å…³æ–‡æ¡£
            if file:
                filter_start = time.time()
                filtered_context = []
                filtered_indices = []
                for ctx, idx in zip(context, indices):
                    if idx >= len(chunks_to_document):
                        continue
                        
                    doc_idx = chunks_to_document[idx]
                    if doc_idx >= len(doc_sources):
                        continue
                        
                    source = doc_sources[doc_idx]
                    if os.path.basename(source) == file:
                        filtered_context.append(ctx)
                        filtered_indices.append(idx)
                
                if filtered_context:
                    context = filtered_context
                    indices = filtered_indices
                else:
                    return JSONResponse(
                        status_code=404,
                        content={"error": f"åœ¨æ–‡ä»¶ {file} ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"}
                    )
                print(f"ğŸ” [æ€§èƒ½è°ƒè¯•] æ–‡ä»¶è¿‡æ»¤å®Œæˆ: {time.time() - filter_start:.3f}s")
            
            web_search_context = ""  # ä¸ä½¿ç”¨ç½‘ç»œæœç´¢
            generate_start = time.time()
            response = await generate_answer(client, question, context, conversation_context, model_name, web_search_context)
            print(f"ğŸ¤– [æ€§èƒ½è°ƒè¯•] ç”Ÿæˆç­”æ¡ˆè°ƒç”¨å®Œæˆ: {time.time() - generate_start:.3f}s")
        
        print(f"âš¡ [æ€§èƒ½è°ƒè¯•] æœåŠ¡å¤„ç†æ€»è€—æ—¶: {time.time() - service_start:.3f}s")
        print(f"ğŸ¯ [æ€§èƒ½è°ƒè¯•] è¯·æ±‚é¢„å¤„ç†æ€»è€—æ—¶: {time.time() - total_start_time:.3f}s")
        
        async def generate():
            try:
                stream_start = time.time()
                first_chunk_time = None
                chunk_count = 0
                full_answer = ""
                buffer = ""  # æ·»åŠ ç¼“å†²åŒº
                buffer_size = 10  # å¢åŠ æ‰¹é‡å‘é€å­—ç¬¦æ•°åˆ°10
                last_send_time = stream_start
                send_interval = 0.1  # 100mså¼ºåˆ¶å‘é€é—´éš”
                
                print(f"ğŸ“¡ [æ€§èƒ½è°ƒè¯•] å¼€å§‹æµå¼å“åº”: {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")
                
                async for chunk in response:
                    if first_chunk_time is None:
                        first_chunk_time = time.time()
                        print(f"âš¡ [æ€§èƒ½è°ƒè¯•] é¦–ä¸ªæ•°æ®å—åˆ°è¾¾: {first_chunk_time - stream_start:.3f}s")
                    
                    if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_answer += content
                        buffer += content
                        chunk_count += 1
                        
                        current_time = time.time()
                        # ä¼˜åŒ–è§¦å‘æ¡ä»¶ï¼šç¼“å†²åŒºæ»¡ã€æµç»“æŸæˆ–æ—¶é—´é—´éš”åˆ°è¾¾
                        should_send = (
                            len(buffer) >= buffer_size or 
                            chunk.choices[0].finish_reason or
                            (current_time - last_send_time) >= send_interval
                        )
                        
                        if should_send and buffer:
                            yield f"data: {json.dumps({'content': buffer}, ensure_ascii=False, separators=(',', ':'))}\n\n"
                            buffer = ""
                            last_send_time = current_time
                
                # å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
                if buffer:
                    yield f"data: {json.dumps({'content': buffer}, ensure_ascii=False, separators=(',', ':'))}\n\n"
                
                stream_end = time.time()
                print(f"ğŸ“Š [æ€§èƒ½è°ƒè¯•] æµå¼å“åº”å®Œæˆ:")
                print(f"   - æ€»è€—æ—¶: {stream_end - stream_start:.3f}s")
                print(f"   - é¦–å—å»¶è¿Ÿ: {(first_chunk_time - stream_start) if first_chunk_time else 0:.3f}s")
                print(f"   - æ•°æ®å—æ•°é‡: {chunk_count}")
                print(f"   - å“åº”é•¿åº¦: {len(full_answer)} å­—ç¬¦")
                print(f"   - å¹³å‡é€Ÿåº¦: {len(full_answer) / (stream_end - stream_start):.1f} å­—ç¬¦/ç§’")
                
                # å¼‚æ­¥ä¿å­˜å®Œæ•´ç­”æ¡ˆåˆ°æ•°æ®åº“
                db_save_start = time.time()
                await db.add_message(conversation_id, full_answer, "system")
                print(f"ğŸ’¾ [æ€§èƒ½è°ƒè¯•] ä¿å­˜AIå›å¤: {time.time() - db_save_start:.3f}s")
                
                # å‘é€å®Œæˆä¿¡å·
                yield f"data: {json.dumps({'done': True, 'conversation_id': conversation_id}, ensure_ascii=False, separators=(',', ':'))}\n\n"
                
                total_end = time.time()
                print(f"ğŸ [æ€§èƒ½è°ƒè¯•] è¯·æ±‚å®Œå…¨ç»“æŸï¼Œæ€»è€—æ—¶: {total_end - total_start_time:.3f}s")
                
            except Exception as e:
                print(f"âŒ [æ€§èƒ½è°ƒè¯•] æµå¼å“åº”é”™è¯¯: {str(e)}")
                yield f"data: {json.dumps({'error': 'ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼Œè¯·é‡è¯•'}, ensure_ascii=False, separators=(',', ':'))}\n\n"
                yield f"data: {json.dumps({'done': True}, ensure_ascii=False, separators=(',', ':'))}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache, no-transform",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Content-Type": "text/event-stream",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "Content-Type",
                "Access-Control-Allow-Methods": "GET, OPTIONS",
                "Access-Control-Allow-Credentials": "true"
            }
        )
        
    except Exception as e:
        error_time = time.time()
        print(f"ğŸ’¥ [æ€§èƒ½è°ƒè¯•] è¯·æ±‚å¼‚å¸¸ï¼Œæ€»è€—æ—¶: {error_time - total_start_time:.3f}s, é”™è¯¯: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"}
        )

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    åŠŸèƒ½ï¼šæ¥æ”¶å¹¶å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè¿›è¡Œå¢é‡å‘é‡åŒ–å’Œç´¢å¼•æ›´æ–°
    å‚æ•°ï¼š
        - file: ä¸Šä¼ çš„æ–‡ä»¶
    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š
        - .pdf, .docx, .doc, .xlsx, .xls, .txt
    è¿”å›ï¼š
        - ä¸Šä¼ å¤„ç†ç»“æœ
    """
    global model, index, all_chunks, doc_sources, chunks_to_document
    allowed_types = {
        '.pdf': 'application/pdf',
        '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        '.doc': 'application/msword',
        '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        '.xls': 'application/vnd.ms-excel',
        '.txt': 'text/plain'
    }
    file_ext = os.path.splitext(file.filename)[1].lower()
    if file_ext not in allowed_types:
        return JSONResponse(
            status_code=400,
            content={'status': 'error', 'message': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}'}
        )
    try:
        upload_dir = Path('uploads')
        upload_dir.mkdir(exist_ok=True)
        file_path = upload_dir / file.filename
        with open(file_path, 'wb') as f:
            content = await file.read()
            f.write(content)
        # å¢é‡å‘é‡åŒ–å’Œç´¢å¼•æ›´æ–°
        if model is None or index is None or all_chunks is None or chunks_to_document is None:
            return JSONResponse(
                status_code=500,
                content={'status': 'error', 'message': 'ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ— æ³•å¢é‡æ’å…¥'}
            )
        # ç»´æŠ¤doc_sources
        if doc_sources is None:
            doc_sources = []
        doc_sources.append(str(file_path))
        doc_source_idx = len(doc_sources) - 1
        ok = add_document_to_index(model, index, all_chunks, chunks_to_document, str(file_path), doc_source_idx)
        if ok:
            return JSONResponse(content={'status': 'success', 'message': 'æ–‡ä»¶ä¸Šä¼ å¹¶å¢é‡å…¥åº“æˆåŠŸ'})
        else:
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œåˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
            if file_path.exists():
                file_path.unlink()
            return JSONResponse(
                status_code=500,
                content={'status': 'error', 'message': 'æ–‡æ¡£å¢é‡å…¥åº“å¤±è´¥'}
            )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={'status': 'error', 'message': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'}
        )

@app.get("/documents")
async def list_documents():
    """
    è·å–æ‰€æœ‰å·²ä¸Šä¼ æ–‡æ¡£çš„åˆ—è¡¨
    åŠŸèƒ½ï¼šè¿”å›uploadsç›®å½•ä¸‹æ‰€æœ‰å·²å¤„ç†çš„æ–‡æ¡£ä¿¡æ¯
    è¿”å›ï¼š
        - æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„å’Œç±»å‹
    """
    try:
        docs = document_loader.load_directory(str(UPLOAD_DIR))
        return {
            "status": "success",
            "documents": [
                {
                    "file_path": doc["file_path"],
                    "file_type": doc["file_type"]
                }
                for doc in docs
            ]
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"
        }

@app.get("/api/conversations")
async def get_conversations():
    """
    è·å–å¯¹è¯åˆ—è¡¨
    åŠŸèƒ½ï¼šè¿”å›æ‰€æœ‰å¯¹è¯çš„åˆ—è¡¨ï¼ŒæŒ‰æ”¶è—çŠ¶æ€å’Œæ›´æ–°æ—¶é—´æ’åº
    è¿”å›ï¼š
        - å¯¹è¯åˆ—è¡¨ï¼ŒåŒ…å«æ¯ä¸ªå¯¹è¯çš„åŸºæœ¬ä¿¡æ¯
    """
    try:
        conversations = await db.get_conversations()
        return JSONResponse(content={
            "status": "success",
            "conversations": [
                {
                    "conversation_id": conv[0],
                    "title": conv[1],
                    "starred": int(conv[2]),  # ç¡®ä¿è½¬æ¢ä¸ºæ•´æ•°
                    "created_at": conv[3],
                    "updated_at": conv[4],
                    "messages": conv[5] if conv[5] else ""
                }
                for conv in conversations
            ]
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

@app.get("/api/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """
    è·å–æŒ‡å®šå¯¹è¯çš„è¯¦ç»†ä¿¡æ¯
    åŠŸèƒ½ï¼šè¿”å›æŒ‡å®šå¯¹è¯IDçš„æ‰€æœ‰æ¶ˆæ¯è®°å½•
    å‚æ•°ï¼š
        - conversation_id: å¯¹è¯ID
    è¿”å›ï¼š
        - æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹ã€ç±»å‹ã€åˆ›å»ºæ—¶é—´ç­‰ä¿¡æ¯
    """
    try:
        messages = await db.get_messages(conversation_id)
        return JSONResponse(content={
            "status": "success",
            "messages": [
                {
                    "id": msg[0],
                    "content": msg[1],
                    "type": msg[2],
                    "created_at": msg[3]
                }
                for msg in messages
            ]
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

@app.delete("/api/conversations/{conversation_id}")
async def delete_conversation(conversation_id: str):
    """
    åˆ é™¤å¯¹è¯
    åŠŸèƒ½ï¼šåˆ é™¤æŒ‡å®šIDçš„å¯¹è¯åŠå…¶æ‰€æœ‰ç›¸å…³æ¶ˆæ¯
    å‚æ•°ï¼š
        - conversation_id: å¯¹è¯ID
    è¿”å›ï¼š
        - åˆ é™¤æ“ä½œç»“æœ
    """
    try:
        success = await db.delete_conversation(conversation_id)
        if success:
            return JSONResponse(content={"status": "success"})
        else:
            return JSONResponse(
                status_code=500,
                content={"status": "error", "message": "åˆ é™¤å¯¹è¯å¤±è´¥"}
            )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

@app.get("/docs_manage", response_class=HTMLResponse)
async def docs_page(request: Request):
    """
    è¿”å›æ–‡æ¡£ç®¡ç†é¡µé¢
    åŠŸèƒ½ï¼šåŠ è½½å¹¶æ˜¾ç¤ºæ–‡æ¡£ç®¡ç†ç•Œé¢
    å‚æ•°ï¼š
        - request: è¯·æ±‚å¯¹è±¡
    è¿”å›ï¼š
        - æ–‡æ¡£ç®¡ç†é¡µé¢HTML
    """
    try:
        return templates.TemplateResponse(
            "docs.html",
            {
                "request": request
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": "åŠ è½½æ–‡æ¡£ç®¡ç†é¡µé¢å¤±è´¥"}
        )

@app.get("/mcp.html", response_class=HTMLResponse)
async def mcp_page(request: Request):
    """
    è¿”å›MCPç®¡ç†é¡µé¢
    åŠŸèƒ½ï¼šåŠ è½½å¹¶æ˜¾ç¤ºMCPæœåŠ¡ç®¡ç†ç•Œé¢
    å‚æ•°ï¼š
        - request: è¯·æ±‚å¯¹è±¡
    è¿”å›ï¼š
        - MCPç®¡ç†é¡µé¢HTML
    """
    try:
        print(f"å°è¯•åŠ è½½MCPé¡µé¢ï¼Œæ¨¡æ¿ç›®å½•: {os.path.join(BASE_DIR, 'templates')}")
        print(f"MCPæ¨¡æ¿æ–‡ä»¶å­˜åœ¨: {os.path.exists(os.path.join(BASE_DIR, 'templates', 'mcp.html'))}")
        
        return templates.TemplateResponse(
            "mcp.html",
            {
                "request": request
            }
        )
    except Exception as e:
        print(f"åŠ è½½MCPç®¡ç†é¡µé¢å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"åŠ è½½MCPç®¡ç†é¡µé¢å¤±è´¥: {str(e)}"}
        )

@app.post("/api/conversations")
async def create_conversation(request: Request):
    """
    åˆ›å»ºæ–°å¯¹è¯
    åŠŸèƒ½ï¼šåˆ›å»ºæ–°çš„å¯¹è¯è®°å½•
    å‚æ•°ï¼š
        - request: åŒ…å«æ ‡é¢˜çš„è¯·æ±‚ä½“
    è¿”å›ï¼š
        - æ–°åˆ›å»ºçš„å¯¹è¯IDå’Œæ ‡é¢˜
    """
    try:
        data = await request.json()
        title = data.get('title', 'New conversation')
        conversation_id = await db.create_conversation(title)
        return JSONResponse(content={
            "status": "success",
            "conversation_id": conversation_id,
            "title": title
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

@app.put("/api/conversations/{conversation_id}")
async def update_conversation(conversation_id: str, request: Request):
    """
    æ›´æ–°å¯¹è¯æ ‡é¢˜
    åŠŸèƒ½ï¼šä¿®æ”¹æŒ‡å®šå¯¹è¯çš„æ ‡é¢˜
    å‚æ•°ï¼š
        - conversation_id: å¯¹è¯ID
        - request: åŒ…å«æ–°æ ‡é¢˜çš„è¯·æ±‚ä½“
    è¿”å›ï¼š
        - æ›´æ–°æ“ä½œç»“æœ
    """
    try:
        data = await request.json()
        new_title = data.get('title')
        if not new_title:
            return JSONResponse(
                status_code=400,
                content={"status": "error", "message": "æ ‡é¢˜ä¸èƒ½ä¸ºç©º"}
            )
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„æ ‡é¢˜
        success = await db.update_conversation_title(conversation_id, new_title)
        if success:
            return JSONResponse(content={
                "status": "success",
                "message": "æ ‡é¢˜æ›´æ–°æˆåŠŸ"
            })
        else:
            return JSONResponse(
                status_code=500,
                content={"status": "error", "message": "æ›´æ–°æ ‡é¢˜å¤±è´¥"}
            )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

@app.put("/api/conversations/{conversation_id}/star")
async def update_conversation_starred(conversation_id: str, request: Request):
    """
    æ›´æ–°å¯¹è¯çš„æ”¶è—çŠ¶æ€
    åŠŸèƒ½ï¼šè®¾ç½®æˆ–å–æ¶ˆå¯¹è¯çš„æ”¶è—çŠ¶æ€
    å‚æ•°ï¼š
        - conversation_id: å¯¹è¯ID
        - request: åŒ…å«starredçŠ¶æ€(1æˆ–0)çš„è¯·æ±‚ä½“
    è¿”å›ï¼š
        - æ›´æ–°æ“ä½œç»“æœ
    """
    try:
        data = await request.json()
        starred = data.get('starred', 0)
        
        success = await db.update_conversation_starred(conversation_id, starred)
        if success:
            return JSONResponse(content={
                "status": "success",
                "message": "æ”¶è—çŠ¶æ€æ›´æ–°æˆåŠŸ"
            })
        else:
            return JSONResponse(
                status_code=500,
                content={
                    "status": "error",
                    "message": "æ›´æ–°æ”¶è—çŠ¶æ€å¤±è´¥"
                }
            )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": str(e)
            }
        )

async def quick_system_check():
    """å¿«é€Ÿç³»ç»Ÿæ£€æŸ¥"""
    global model, index, all_chunks, client
    
    if not system_state.needs_check():
        return True
    
    if model is None or index is None or all_chunks is None:
        return await initialize_system()
    
    system_state.mark_initialized()
    return True

def create_optimized_client(model_name: str) -> AsyncOpenAI:
    """åˆ›å»ºä¼˜åŒ–çš„OpenAIå®¢æˆ·ç«¯"""
    return AsyncOpenAI(
        api_key=MODEL_CONFIG[model_name]["api_key"],
        base_url=MODEL_CONFIG[model_name]["api_base"],
        timeout=60.0,  # è®¾ç½®60ç§’è¶…æ—¶
        max_retries=2,  # æœ€å¤§é‡è¯•2æ¬¡
        http_client=None  # ä½¿ç”¨é»˜è®¤HTTPå®¢æˆ·ç«¯
    )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)




